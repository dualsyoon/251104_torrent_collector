# -*- coding: utf-8 -*-
"""
Sukebei Nyaa ê²€ìƒ‰ â†’ 'ì²« ë²ˆì§¸ ê²°ê³¼(ì œëª© ì—„ê²© ë§¤ì¹­)' ìƒì„¸ í˜ì´ì§€ì—ì„œ
ğŸ”¹ìƒì„¸í˜ì´ì§€ì— 'ì´ë¯¸ì§€ í˜•íƒœ(= ì‹¤ì œ <img>)'ë¡œ ë“¤ì–´ìˆëŠ” URLë§Œ ì¶”ì¶œ(í•„ìˆ˜)
ğŸ”¹ì¸ë„¤ì¼ ì—…ê·¸ë ˆì´ë“œ( *_t.jpg â†’ .jpg ) ì—†ìŒ  â† ìš”êµ¬ì‚¬í•­
ğŸ”¹HTML ë·°ì–´(.html) ë”°ë¼ê°€ì§€ ì•ŠìŒ         â† ìš”êµ¬ì‚¬í•­

ê¸°ë³¸ ê·œì¹™
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—¬ëŸ¬ ê°œì—¬ë„ 'ì²« ë²ˆì§¸ ê²°ê³¼'ë§Œ ì²˜ë¦¬.
- ê·¸ ê²°ê³¼ì˜ ì œëª©ì´ í‚¤ì›Œë“œì™€ 'ì—„ê²© ë§¤ì¹­'ì¼ ë•Œë§Œ ì§„í–‰.
  (ë¬¸ì+ìˆ«ìê°€ ì •í™•íˆ ê°™ê³ , ë¬¸ì/ìˆ«ì ì‚¬ì´ì˜ '-' ë§Œ ì˜µì…˜. ì˜ˆ: STARS-080 â†” STARS080)
- ìƒì„¸ í˜ì´ì§€ì˜ 'ì„¤ëª…(Description)' ì˜ì—­ ì¤‘ì‹¬ìœ¼ë¡œ 'ë Œë”ëœ ì´ë¯¸ì§€(<img>)'ë§Œ ìˆ˜ì§‘:
  - <img>ì˜ src / data-src / data-original / srcset
  - (ì„¤ëª…ì´ ë§ˆí¬ë‹¤ìš´ ì›ë¬¸ì¼ ê²½ìš°) [![](IMG)](...)ì˜ IMGë§Œ ì¸ì‹, LINKëŠ” ë¬´ì‹œ
- ìì‚°/ì•„ì´ì½˜(logo/favicon/icon/ads ë“±) ì œì™¸, image/* ë§Œ í—ˆìš©, ìµœì†Œ ìš©ëŸ‰(MIN_BYTES) í•„í„°
- ë””ë²„ê·¸ JSONì„ out_dir/debug_<tag>_<ts>.jsonìœ¼ë¡œ ì €ì¥

ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
- scrape_nyaa_image_urls_by_keyword(keyword, out_dir="downloads", download=False)
  â†’ ê²€ìƒ‰ì–´ë¡œ ìƒì„¸ í˜ì´ì§€ë¥¼ ì°¾ì•„ê°€ê³ , ìƒì„¸í˜ì´ì§€ì— 'ë³´ì´ëŠ”' ì´ë¯¸ì§€ URLë§Œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(+ì„ íƒ ì €ì¥)
"""

import os
import re
import json
import time
import pathlib
import urllib.parse as up
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# -------------------- ì„¤ì • --------------------
BASE_SEARCH = "https://sukebei.nyaa.si/"
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Python-requests"
MIN_BYTES = 10 * 1024  # 10KB (Content-Length ìˆìœ¼ë©´ ìµœì†Œ ìš©ëŸ‰, ì—†ìœ¼ë©´ ë¬´ì‹œ)
IMG_EXTS = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".bmp", ".avif"}

# ì¸ë„¤ì¼/í”„ë¡ì‹œ/ìì‚° í•„í„°
THUMB_HOSTS = ("i0.wp.com", "i1.wp.com", "i2.wp.com")
ASSET_SEG_RE = re.compile(
    r"(?:^|/)(?:logo|favicon|sprite|icons?|ads?|banners?|static|assets|themes|emoji|svg)(?:/|$)",
    re.I
)

# ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€+ë§í¬ ìŒ (IMGì™€ LINKë¥¼ ë¶„ë¦¬. ìš°ë¦¬ëŠ” IMGë§Œ ì‚¬ìš©)
MD_IMG_LINK_RE = re.compile(
    r'\[!\[[^\]]*\]\((?P<img>https?://[^\s\)\]]+)\)\]\((?P<link>https?://[^\s\)\]]+)\)',
    re.I
)

# -------------------- í—¤ë”/ìœ í‹¸ --------------------
def make_headers(referer: str | None = None) -> dict:
    h = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,ko;q=0.8",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Upgrade-Insecure-Requests": "1",
        "DNT": "1",
    }
    if referer:
        h["Referer"] = referer
    return h

def is_probably_asset(u: str) -> bool:
    path = up.urlparse(u).path
    return bool(ASSET_SEG_RE.search(path))  # uploads ì—¬ë¶€ì™€ ë¬´ê´€, ì„¸ê·¸ë¨¼íŠ¸ë¡œ íŒë‹¨

def compile_keyword_strict(keyword: str) -> re.Pattern:
    """
    'ë¬¸ì+ìˆ«ì'ê°€ ì •í™•íˆ ê°™ê³ , ë¬¸ì/ìˆ«ì ì‚¬ì´ì˜ '-' ë§Œ ì˜µì…˜.
    ì˜ˆ) 'STARS-080' -> (?<!alnum)STARS-?080(?!alnum)
    """
    m = re.match(r"^\s*([A-Za-z]+)\s*-?\s*(\d+)\s*$", keyword)
    if not m:
        k = keyword.strip()
        k = re.escape(k).replace(r"\-", "-?")
        return re.compile(rf"(?<![A-Za-z0-9]){k}(?![A-Za-z0-9])", re.I)
    prefix, num = m.groups()
    return re.compile(
        rf"(?<![A-Za-z0-9]){re.escape(prefix)}-?{re.escape(num)}(?![A-Za-z0-9])",
        re.I,
    )

def urljoin(base, url):
    return up.urljoin(base, url)

def ext_from_content_type(ct: str) -> str:
    ct = (ct or "").lower()
    if "jpeg" in ct: return ".jpg"
    if "png"  in ct: return ".png"
    if "webp" in ct: return ".webp"
    if "gif"  in ct: return ".gif"
    if "bmp"  in ct: return ".bmp"
    if "avif" in ct: return ".avif"
    return ".jpg"

def head_or_small_get(url: str, session: requests.Session, referer: str):
    headers = make_headers(referer)
    try:
        r = session.head(url, headers=headers, allow_redirects=True, timeout=15)
        ct = (r.headers.get("content-type") or "").lower()
        cl = r.headers.get("content-length")
        size = int(cl) if cl and cl.isdigit() else None
        return {"ok": ct.startswith("image/"), "final_url": r.url, "ct": ct, "size": size}
    except Exception:
        pass
    try:
        with session.get(url, headers=headers, stream=True, allow_redirects=True, timeout=25) as g:
            ct = (g.headers.get("content-type") or "").lower()
            cl = g.headers.get("content-length")
            size = int(cl) if cl and cl.isdigit() else None
            return {"ok": ct.startswith("image/"), "final_url": g.url, "ct": ct, "size": size}
    except Exception:
        return {"ok": False, "final_url": url, "ct": "", "size": None}

def get_html(url: str, session: requests.Session, referer: str | None = None) -> str:
    r = session.get(url, headers=make_headers(referer), timeout=25)
    r.raise_for_status()
    return r.text

# -------------------- ê²€ìƒ‰/íŒŒì‹± --------------------
def build_search_url(keyword: str) -> str:
    qs = up.urlencode({"f": 0, "c": "0_0", "q": keyword})
    return f"{BASE_SEARCH}?{qs}"

def find_first_result_and_title(soup: BeautifulSoup):
    """
    ê²€ìƒ‰ ê²°ê³¼ í…Œì´ë¸”ì—ì„œ ê°€ì¥ ë¨¼ì € ë‚˜ì˜¤ëŠ” '/view/xxxx' ë§í¬ì™€ ì œëª© í…ìŠ¤íŠ¸ë¥¼ ì°¾ìŒ.
    """
    a = soup.select_one("td a[href^='/view/'], a[href^='/view/']")
    if not a:
        return None, None
    title = (a.get_text(" ", strip=True) or "")[:500]
    href = a.get("href")
    if not href:
        return None, None
    return urljoin(BASE_SEARCH, href), title

def find_description_nodes(soup: BeautifulSoup):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ì„¤ëª…'ì— í•´ë‹¹í•˜ëŠ” ì»¨í…Œì´ë„ˆ í›„ë³´ë“¤ì„ ì°¾ëŠ”ë‹¤.
    - id ìš°ì„ : #torrent-description, #description
    - íŒ¨ë„ êµ¬ì¡°: 'Description' í—¤ë”ë¥¼ ê°€ì§„ panelì˜ body
    - ë°±ì—…: ê¸°ì‚¬/ë³¸ë¬¸ìŠ¤ëŸ¬ìš´ ë¸”ë¡ ëª‡ ê°œ
    """
    nodes = []
    nodes.extend(soup.select("#torrent-description, #description"))
    for panel in soup.select(".panel"):
        header = panel.select_one(".panel-heading")
        if header and re.search(r"\bdescription\b", header.get_text(" ", strip=True), re.I):
            body = panel.select_one(".panel-body") or panel
            nodes.append(body)
    if not nodes:
        nodes.extend(soup.select("article"))
    if not nodes:
        nodes.extend(soup.select("div.content, .content, .container"))
    seen, uniq = set(), []
    for n in nodes:
        k = str(n)
        if k not in seen:
            uniq.append(n); seen.add(k)
    return uniq[:3]

# -------------------- ê³µí†µ ì €ì¥/ë¡œê¹… --------------------
def save_debug_json(out_dir: str, tag: str, debug: dict):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = os.path.join(out_dir, f"debug_{tag}_{ts}.json")
    os.makedirs(out_dir, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(debug, f, ensure_ascii=False, indent=2)
    print(f"[debug] log saved -> {path}")

# -------------------- í•µì‹¬: ìƒì„¸í˜ì´ì§€ì—ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ ì¶”ì¶œ --------------------
def extract_visible_image_urls_from_view(
    view_url: str,
    out_dir: str = "downloads",
    download: bool = False,             # ê¸°ë³¸ì€ URLë§Œ ì¶œë ¥(ìš”êµ¬: "urlì¶œë ¥")
    session: requests.Session | None = None,
    referer: str | None = None,
) -> list[str]:
    """
    ìƒì„¸í˜ì´ì§€ì— 'ë³´ì´ëŠ”' ì´ë¯¸ì§€ URLë§Œ ë°˜í™˜(+ì„ íƒ ì €ì¥).
    - <img>ì˜ src / data-src / data-original / srcset ë§Œ ëŒ€ìƒ
    - ë§ˆí¬ë‹¤ìš´ì´ ì›ë¬¸ìœ¼ë¡œ ìˆì„ ë•ŒëŠ” [![](IMG)](...) ì˜ IMGë§Œ ì¶”ì¶œ (LINKëŠ” ë¬´ì‹œ)
    - a[href]ì˜ .html ë·°ì–´ëŠ” ë”°ë¥´ì§€ ì•ŠìŒ
    - í”„ë¡ì‹œ deproxy, ì¸ë„¤ì¼ ì—…ê·¸ë ˆì´ë“œ( *_t â†’ ì›ë³¸ ) ê°™ì€ ì¡°ì‘ ì—†ìŒ
    - image/* ë§Œ í—ˆìš©, MIN_BYTES í•„í„°, ìì‚°(logo/icon/ads ë“±) ì œì™¸
    """
    debug = {"view_url": view_url, "mode": "visible_imgs_only", "min_bytes": MIN_BYTES}
    s = session or requests.Session()
    results = []

    # ìƒì„¸ HTML ë¡œë”©
    html = get_html(view_url, s, referer=referer or BASE_SEARCH)
    dsoup = BeautifulSoup(html, "lxml")
    debug["detail_html_len"] = len(html)

    # ì„¤ëª… ì˜ì—­ ìš°ì„  + ë°±ì—…
    nodes = find_description_nodes(dsoup)
    if not nodes:
        nodes = [dsoup]

    # 1) DOM ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ í›„ë³´ ìˆ˜ì§‘(<img>ë§Œ!)
    raw = []
    def add_img_url(u, how):
        if u:
            raw.append({"url": urljoin(view_url, u), "how": how})

    for n in nodes:
        for img in n.find_all("img"):
            add_img_url(img.get("src"), "img.src")
            add_img_url(img.get("data-src"), "img.data-src")
            add_img_url(img.get("data-original"), "img.data-original")
            srcset = img.get("srcset")
            if srcset:
                parts = [p.strip() for p in srcset.split(",") if p.strip()]
                for p in parts:
                    add_img_url(p.split()[0], "img.srcset")

    # 2) ë§ˆí¬ë‹¤ìš´ì´ ì›ë¬¸ìœ¼ë¡œ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ [![](IMG)](LINK) â†’ IMGë§Œ ì¶”ê°€
    for m in MD_IMG_LINK_RE.finditer(html or ""):
        img_url = m.group("img")
        if img_url:
            add_img_url(img_url, "md.img")

    # ì¤‘ë³µ ì œê±°
    seen, cands = set(), []
    for it in raw:
        u = it["url"]
        if u not in seen:
            cands.append(it); seen.add(u)

    debug["candidate_count"] = len(cands)
    debug["candidates_sample"] = cands[:20]

    accepted, rejected = [], []

    # ì €ì¥ íŒŒì¼ëª… prefix (ë·° ID ì¶”ì¶œ)
    view_id = re.search(r"/view/(\d+)", view_url)
    base_prefix = f"view{view_id.group(1)}" if view_id else "view"

    for i, item in enumerate(cands, 1):
        u, how = item["url"], item["how"]

        # a) ìì‚°/ì•„ì´ì½˜ ì œì™¸
        if is_probably_asset(u):
            rejected.append({"url": u, "reason": "asset_segment", "how": how})
            continue

        # b) URLì´ .html ë“± ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ ì œì™¸ (ìš°ë¦¬ëŠ” <img>ë§Œ ìˆ˜ì§‘í–ˆìœ¼ë‚˜ ì•ˆì „ì¥ì¹˜)
        ext = pathlib.Path(up.urlparse(u).path).suffix.lower()
        if ext == ".html":
            rejected.append({"url": u, "reason": "html_viewer_not_allowed", "how": how})
            continue

        # c) ë„¤íŠ¸ì›Œí¬ ê²€ì‚¬: image/* ë§Œ í—ˆìš© + ìµœì†Œ ìš©ëŸ‰
        probe = head_or_small_get(u, s, referer=view_url)
        if not probe["ok"]:
            rejected.append({"url": u, "reason": f"not_image({probe['ct']})", "how": how})
            continue
        size_ok = (probe["size"] is None) or (probe["size"] >= MIN_BYTES)
        if not size_ok:
            rejected.append({"url": u, "reason": f"small({probe['size']})", "how": how})
            continue

        # d) í†µê³¼ â†’ URL ì¶œë ¥(í•„ìˆ˜), í•„ìš” ì‹œ ì €ì¥
        results.append(u)
        if download:
            name = pathlib.Path(up.urlparse(u).path).name or f"{base_prefix}_{i:02d}{ext_from_content_type(probe['ct'])}"
            dest = os.path.join(out_dir, name)
            os.makedirs(out_dir, exist_ok=True)
            try:
                with s.get(u, headers=make_headers(view_url), stream=True, timeout=40) as r:
                    r.raise_for_status()
                    with open(dest, "wb") as f:
                        for chunk in r.iter_content(8192):
                            if chunk: f.write(chunk)
                accepted.append({"url": u, "saved": dest, "ct": probe["ct"], "size": probe["size"], "how": how})
                print(f"saved: {dest}")
                time.sleep(0.08)
            except Exception as e:
                rejected.append({"url": u, "reason": f"download_error:{e}", "how": how})
        else:
            accepted.append({"url": u, "ct": probe["ct"], "size": probe["size"], "how": how})

    debug["accepted"] = accepted
    debug["rejected"] = rejected
    debug["returned_count"] = len(results)
    save_debug_json(out_dir, f"view_{view_id.group(1) if view_id else 'manual'}", debug)

    return results

# -------------------- ì—”íŠ¸ë¦¬: ê²€ìƒ‰ì–´ë¡œ ìƒì„¸ ì°¾ì•„ê°€ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ --------------------
def scrape_nyaa_image_urls_by_keyword(
    keyword: str,
    out_dir: str = "downloads",
    download: bool = False  # ê¸°ë³¸ False: "url ì¶œë ¥" ì¤‘ì‹¬(ìš”êµ¬ì‚¬í•­)
) -> list[str]:
    """
    1) ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ í˜ì´ì§€ ì§„ì…
    2) 'ì²« ë²ˆì§¸ ê²°ê³¼'ì˜ ì œëª©ì„ í‚¤ì›Œë“œì™€ ì—„ê²© ë§¤ì¹­ ì²´í¬
    3) í†µê³¼ ì‹œ ìƒì„¸(view) í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ ì¶”ì¶œ(+ì„ íƒ ì €ì¥)
    """
    debug = {"keyword": keyword, "base": BASE_SEARCH, "mode": "by_keyword_visible_imgs", "min_bytes": MIN_BYTES}
    kw_re = compile_keyword_strict(keyword)

    with requests.Session() as s:
        # 1) ê²€ìƒ‰
        search_url = build_search_url(keyword)
        html = get_html(search_url, s, referer=BASE_SEARCH)
        debug["search_url"] = search_url
        debug["search_html_len"] = len(html)

        soup = BeautifulSoup(html, "lxml")
        view_url, title_text = find_first_result_and_title(soup)
        debug["view_url"] = view_url
        debug["title_text"] = title_text

        # 2) ì œëª© ì—„ê²© ë§¤ì¹­
        if not view_url or not title_text:
            debug["error"] = "no_result_view_link"
            save_debug_json(out_dir, keyword, debug)
            return []

        if not kw_re.search(title_text or ""):
            debug["error"] = "title_not_match_first_result"
            save_debug_json(out_dir, keyword, debug)
            return []

        # 3) ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€'ë§Œ ì¶”ì¶œ
        urls = extract_visible_image_urls_from_view(
            view_url,
            out_dir=out_dir,
            download=download,
            session=s,
            referer=search_url
        )

    # ìš”ì•½ ë””ë²„ê·¸
    save_debug_json(out_dir, f"keyword_{keyword}", {
        "keyword": keyword,
        "view_url": view_url,
        "download": download,
        "found_count": len(urls)
    })
    return urls

# -------------------- ì‹¤í–‰ ì˜ˆì‹œ --------------------
if __name__ == "__main__":
    # ì˜ˆ) ê²€ìƒ‰ì–´ë¡œ ìƒì„¸ ì°¾ì•„ê°€ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ ì¶œë ¥
    urls = scrape_nyaa_image_urls_by_keyword(
        "4017-XXX",          # ì˜ˆì‹œ í‚¤ì›Œë“œ
        out_dir="test_images",
        download=False       # URLë§Œ(ì €ì¥ X)
    )
    print("URLS ON PAGE (IMG ONLY):", *urls, sep="\n")
