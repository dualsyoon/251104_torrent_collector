# -*- coding: utf-8 -*-
"""
JAVMOST(www5.javmost.com) ê²€ìƒ‰ â†’ 'ì²« ë²ˆì§¸ ê²°ê³¼(ì œëª© ì—„ê²© ë§¤ì¹­)' ìƒì„¸ í˜ì´ì§€ì—ì„œ
ğŸ”¹ìƒì„¸í˜ì´ì§€ì— 'ì´ë¯¸ì§€ í˜•íƒœ(= ì‹¤ì œ <img>)'ë¡œ ë“¤ì–´ìˆëŠ” URLë§Œ ì¶”ì¶œ(í•„ìˆ˜)
ğŸ”¹ì¸ë„¤ì¼ ì—…ê·¸ë ˆì´ë“œ( *_t.jpg â†’ .jpg ) ì—†ìŒ  â† ìš”êµ¬ì‚¬í•­
ğŸ”¹HTML ë·°ì–´(.html) ë”°ë¼ê°€ì§€ ì•ŠìŒ         â† ìš”êµ¬ì‚¬í•­

ê¸°ë³¸ ê·œì¹™
- ê²€ìƒ‰ ê²°ê³¼ê°€ ì—¬ëŸ¬ ê°œì—¬ë„ 'ì²« ë²ˆì§¸ ê²°ê³¼'ë§Œ ì²˜ë¦¬.
- ê·¸ ê²°ê³¼ì˜ ì œëª©ì´ í‚¤ì›Œë“œì™€ 'ì—„ê²© ë§¤ì¹­'ì¼ ë•Œë§Œ ì§„í–‰.
  (ë¬¸ì+ìˆ«ìê°€ ì •í™•íˆ ê°™ê³ , ë¬¸ì/ìˆ«ì ì‚¬ì´ì˜ '-' ë§Œ ì˜µì…˜. ì˜ˆ: STARS-080 â†” STARS080)
- ìƒì„¸ í˜ì´ì§€ì˜ 'ë³´ì´ëŠ” ì´ë¯¸ì§€(<img>)'ë§Œ ìˆ˜ì§‘:
  - <img>ì˜ src / data-* / srcset
  - (ì„¤ëª…ì´ ë§ˆí¬ë‹¤ìš´ ì›ë¬¸ì¼ ê²½ìš°) [![](IMG)](...)ì˜ IMGë§Œ ì¸ì‹, LINKëŠ” ë¬´ì‹œ
- ìì‚°/ì•„ì´ì½˜(logo/favicon/icon/ads ë“±) ì œì™¸, image/* ë§Œ í—ˆìš©, ìµœì†Œ ìš©ëŸ‰(MIN_BYTES) í•„í„°
- ë””ë²„ê·¸ JSONì„ out_dir/debug_<tag>_<ts>.json ìœ¼ë¡œ ì €ì¥

ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
- scrape_javmost_image_urls_by_keyword(keyword, out_dir="downloads", download=False, **extract_opts)
  â†’ ê²€ìƒ‰ì–´ë¡œ ìƒì„¸ í˜ì´ì§€ë¥¼ ì°¾ì•„ê°€ê³ , ìƒì„¸í˜ì´ì§€ì— 'ë³´ì´ëŠ”' ì´ë¯¸ì§€ URLë§Œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(+ì„ íƒ ì €ì¥)

ì£¼ì˜
- JAVMOSTëŠ” ëŒ€í‘œ í¬ìŠ¤í„°ë¥¼ ë©”íƒ€íƒœê·¸ë‚˜ JSë¡œë§Œ ë…¸ì¶œí•˜ëŠ” ê²½ìš°ê°€ ìˆì–´ ê¸°ë³¸ ì˜µì…˜ìœ¼ë¡œ
  og/twitter ì´ë¯¸ì§€ í´ë°±(include_meta=True)ê³¼ í¬ìŠ¤í„° ì¶”ì •(poster_guess=True)ì„ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤.
  ì—„ê²©íˆ "<img>ë§Œ" ì›í•˜ì‹œë©´ ë‘ ì˜µì…˜ì„ False ë¡œ ë°”ê¿” ì“°ì„¸ìš”.
"""

import os
import re
import json
import time
import pathlib
import urllib.parse as up
from datetime import datetime

import requests
from bs4 import BeautifulSoup

# -------------------- ì„¤ì • --------------------
BASE = "https://www5.javmost.com/"
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Python-requests"
MIN_BYTES = 10 * 1024  # 10KB (Content-Length ìˆìœ¼ë©´ ìµœì†Œ ìš©ëŸ‰, ì—†ìœ¼ë©´ ë¬´ì‹œ)
IMG_EXTS = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".bmp", ".avif"}

# ì¸ë„¤ì¼/í”„ë¡ì‹œ/ìì‚°/ê´‘ê³  í•„í„°
THUMB_HOSTS = ("i0.wp.com", "i1.wp.com", "i2.wp.com")
ASSET_SEG_RE = re.compile(
    r"(?:^|/)(?:logo|favicon|sprite|icons?|ads?|adserver|banners?|static|assets|themes|emoji|svg)(?:/|$)",
    re.I
)
AD_HOST_RE = re.compile(r"(?:exosrv|exdynsrv|syndication|doubleclick|adnxs|taboola|outbrain|histats)", re.I)

# ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€+ë§í¬ ìŒ (IMGë§Œ ì‚¬ìš©)
MD_IMG_LINK_RE = re.compile(
    r'\[!\[[^\]]*\]\((?P<img>https?://[^\s\)\]]+)\)\]\((?P<link>https?://[^\s\)\]]+)\)',
    re.I
)

# -------------------- í—¤ë”/ìœ í‹¸ --------------------
def make_headers(referer: str | None = None) -> dict:
    h = {
        "User-Agent": UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9,ko;q=0.8",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Upgrade-Insecure-Requests": "1",
        "DNT": "1",
    }
    if referer:
        h["Referer"] = referer
    return h

def is_probably_asset(u: str) -> bool:
    pr = up.urlparse(u)
    if AD_HOST_RE.search(pr.netloc):  # ê´‘ê³ /íŠ¸ë˜í‚¹ ë„ë©”ì¸ ì»·
        return True
    path = pr.path
    if any(host in pr.netloc for host in THUMB_HOSTS):
        return True
    return bool(ASSET_SEG_RE.search(path))

def compile_keyword_strict(keyword: str) -> re.Pattern:
    """
    'ë¬¸ì+ìˆ«ì'ê°€ ì •í™•íˆ ê°™ê³ , ë¬¸ì/ìˆ«ì ì‚¬ì´ì˜ '-' ë§Œ ì˜µì…˜.
    ì˜ˆ) 'STARS-080' -> (?<!alnum)STARS-?080(?!alnum)
    """
    m = re.match(r"^\s*([A-Za-z]+)\s*-?\s*(\d+)\s*$", keyword)
    if not m:
        k = keyword.strip()
        k = re.escape(k).replace(r"\-", "-?")
        return re.compile(rf"(?<![A-Za-z0-9]){k}(?![A-Za-z0-9])", re.I)
    prefix, num = m.groups()
    return re.compile(
        rf"(?<![A-Za-z0-9]){re.escape(prefix)}-?{re.escape(num)}(?![A-Za-z0-9])",
        re.I,
    )

def normalize_code(keyword: str) -> tuple[str | None, str | None, str | None]:
    """
    í‚¤ì›Œë“œë¥¼ 'PREFIX-NNN' í˜•íƒœë¡œ ì •ê·œí™”í•˜ì—¬ (prefix, num, code) íŠœí”Œ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ (None, None, None)
    """
    m = re.match(r"^\s*([A-Za-z]+)\s*-?\s*(\d+)\s*$", keyword)
    if not m:
        return None, None, None
    prefix, num = m.groups()
    code = f"{prefix.upper()}-{num}"
    return prefix.upper(), num, code

def urljoin(base, url):
    return up.urljoin(base, url)

def ext_from_content_type(ct: str) -> str:
    ct = (ct or "").lower()
    if "jpeg" in ct: return ".jpg"
    if "png"  in ct: return ".png"
    if "webp" in ct: return ".webp"
    if "gif"  in ct: return ".gif"
    if "bmp"  in ct: return ".bmp"
    if "avif" in ct: return ".avif"
    return ".jpg"

def head_or_small_get(url: str, session: requests.Session, referer: str):
    headers = make_headers(referer)
    try:
        r = session.head(url, headers=headers, allow_redirects=True, timeout=15)
        ct = (r.headers.get("content-type") or "").lower()
        cl = r.headers.get("content-length")
        size = int(cl) if cl and cl.isdigit() else None
        return {"ok": ct.startswith("image/"), "final_url": r.url, "ct": ct, "size": size}
    except Exception:
        pass
    try:
        with session.get(url, headers=headers, stream=True, allow_redirects=True, timeout=25) as g:
            ct = (g.headers.get("content-type") or "").lower()
            cl = g.headers.get("content-length")
            size = int(cl) if cl and cl.isdigit() else None
            return {"ok": ct.startswith("image/"), "final_url": g.url, "ct": ct, "size": size}
    except Exception:
        return {"ok": False, "final_url": url, "ct": "", "size": None}

def get_html(url: str, session: requests.Session, referer: str | None = None) -> str:
    r = session.get(url, headers=make_headers(referer), timeout=25)
    r.raise_for_status()
    return r.text

# -------------------- JAVMOST ê²€ìƒ‰/í•´ê²° --------------------
def try_direct_view(session: requests.Session, code: str) -> tuple[str | None, str | None]:
    """
    /<CODE>/ ì§í–‰ ì‹œë„. ì„±ê³µí•˜ë©´ (view_url, title_text) ë°˜í™˜.
    """
    view = urljoin(BASE, f"{code}/")
    try:
        html = get_html(view, session, referer=BASE)
        soup = BeautifulSoup(html, "lxml")
        title = ""
        if soup.title and soup.title.get_text():
            title = soup.title.get_text(" ", strip=True)
        h = soup.find(["h1","h2"])
        if not title and h:
            title = h.get_text(" ", strip=True)
        return view, (title or "")
    except Exception:
        return None, None

def find_from_tag_listing(session: requests.Session, prefix: str, kw_re: re.Pattern) -> tuple[str | None, str | None]:
    """
    /tag/<PREFIX>/ ëª©ë¡ì—ì„œ 'ì²«ë²ˆì§¸ ì—„ê²©ë§¤ì¹­' í¬ìŠ¤íŠ¸ ë§í¬ë¥¼ ì°¾ëŠ”ë‹¤.
    """
    tag_url = urljoin(BASE, f"tag/{prefix}/")
    html = get_html(tag_url, session, referer=BASE)
    soup = BeautifulSoup(html, "lxml")

    # a[href]ë“¤ ì¤‘ì—ì„œ í…ìŠ¤íŠ¸ê°€ ì½”ë“œì— 'ì—„ê²©ë§¤ì¹­'ë˜ëŠ” ì²« ë²ˆì§¸ë¥¼ ì±„íƒ.
    for a in soup.select("a[href]"):
        txt = (a.get_text(" ", strip=True) or "")[:500]
        href = a.get("href") or ""
        if not href:
            continue
        # ë‚´ë¹„/íƒœê·¸/ë°°ë„ˆ/í˜ì´ì§€ë„¤ì´ì…˜ ì œì™¸
        if any(seg in href for seg in ("/tag/", "/maker/", "/director/", "/category/", "/search/", "/allcode/")):
            continue
        if kw_re.search(txt):
            return urljoin(BASE, href), txt
    return None, None

def resolve_view_url_and_title(keyword: str, session: requests.Session) -> tuple[str | None, str | None, dict]:
    """
    í‚¤ì›Œë“œë¡œ ìƒì„¸(view) URLê³¼ ì œëª©ì„ ì°¾ì•„ì¤€ë‹¤.
    1) /<CODE>/ ì§í–‰
    2) /tag/<PREFIX>/ ëª©ë¡ì—ì„œ ì²« ë§¤ì¹­
    """
    debug_steps = {}
    kw_re = compile_keyword_strict(keyword)
    prefix, num, code = normalize_code(keyword)

    # 1) ì§ì ‘ ìŠ¬ëŸ¬ê·¸
    if code:
        v, t = try_direct_view(session, code)
        debug_steps["direct_code"] = {"code": code, "found": bool(v)}
        if v and kw_re.search((t or "")):
            return v, t, debug_steps

    # 2) íƒœê·¸ ëª©ë¡ ê²€ìƒ‰
    if prefix:
        v, t = find_from_tag_listing(session, prefix, kw_re)
        debug_steps["tag_listing"] = {"prefix": prefix, "found": bool(v)}
        if v and t and kw_re.search(t):
            return v, t, debug_steps

    return None, None, debug_steps

# -------------------- ì„¤ëª…/ì½˜í…ì¸  ì˜ì—­ í›„ë³´ --------------------
def find_description_nodes(soup: BeautifulSoup):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ì£¼ ì½˜í…ì¸  ì˜ì—­ í›„ë³´ë¥¼ ì°¾ëŠ”ë‹¤.
    - JAVMOSTëŠ” ëª…ì‹œ 'description' ì•„ì´ë””ê°€ ì—†ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ
      ë³¸ë¬¸/ì•„í‹°í´/ë©”ì¸ ì»¨í…Œì´ë„ˆ ìœ„ì£¼ë¡œ ìŠ¤ìº”
    """
    nodes = []
    # í”í•œ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë“¤
    nodes.extend(soup.select("article, main, section"))
    nodes.extend(soup.select("div.post, div.single, div.entry-content, div.content, .container"))
    if not nodes:
        nodes = [soup]
    # ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for n in nodes:
        k = str(n)
        if k not in seen:
            uniq.append(n); seen.add(k)
    return uniq[:5]

# -------------------- ê³µí†µ ì €ì¥/ë¡œê¹… --------------------
def save_debug_json(out_dir: str, tag: str, debug: dict):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = os.path.join(out_dir, f"debug_{tag}_{ts}.json")
    os.makedirs(out_dir, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(debug, f, ensure_ascii=False, indent=2)
    print(f"[debug] log saved -> {path}")

# -------------------- í•µì‹¬: ìƒì„¸í˜ì´ì§€ì—ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ ì¶”ì¶œ --------------------
def extract_visible_image_urls_from_view(
    view_url: str,
    out_dir: str = "downloads",
    download: bool = False,             # ê¸°ë³¸ì€ URLë§Œ ì¶œë ¥
    session: requests.Session | None = None,
    referer: str | None = None,
    include_noscript: bool = True,      # lazy-load noscript ë‚´ <img> í—ˆìš©(ê¶Œì¥)
    include_meta: bool = True,          # og:image ë“± ë©”íƒ€ í´ë°±(ê¸°ë³¸ ON) - í•„ìš”ì‹œ False
    include_script: bool = False,       # <script> ë‚´ ì´ë¯¸ì§€ URL í´ë°±(ê¸°ë³¸ OFF)
    include_video_poster: bool = False, # <video poster> í—ˆìš©(ê¸°ë³¸ OFF)
    poster_guess: bool = True,          # JAVMOST ì „ìš© í¬ìŠ¤í„° ì¶”ì •(ê¸°ë³¸ ON)
) -> list[str]:
    """
    ìƒì„¸í˜ì´ì§€ì— 'ë³´ì´ëŠ”' ì´ë¯¸ì§€ URLë§Œ ë°˜í™˜(+ì„ íƒ ì €ì¥).
    - <img>ì˜ src / data-* / srcset ëŒ€ìƒ
    - [![](IMG)](LINK) â†’ IMGë§Œ ì¶”ì¶œ (LINKëŠ” ë¬´ì‹œ)
    - a[href]ì˜ .html ë·°ì–´ëŠ” ë”°ë¥´ì§€ ì•ŠìŒ
    - í”„ë¡ì‹œ deproxy, ì¸ë„¤ì¼ ì—…ê·¸ë ˆì´ë“œ ê°™ì€ ì¡°ì‘ ì—†ìŒ
    - image/* ë§Œ í—ˆìš©, MIN_BYTES í•„í„°, ìì‚°/ê´‘ê³  ì œì™¸
    - (ì˜µì…˜) og/twitter ë©”íƒ€ í´ë°±, (ì˜µì…˜) í¬ìŠ¤í„° ì¶”ì •
    """
    debug = {
        "view_url": view_url,
        "mode": "visible_imgs_only",
        "min_bytes": MIN_BYTES,
        "opts": {
            "include_noscript": include_noscript,
            "include_meta": include_meta,
            "include_script": include_script,
            "include_video_poster": include_video_poster,
            "poster_guess": poster_guess,
        }
    }
    s = session or requests.Session()
    results, raw = [], []

    def add_img_url(u, how):
        if u:
            raw.append({"url": urljoin(view_url, u), "how": how})

    # ìƒì„¸ HTML ë¡œë”©
    html = get_html(view_url, s, referer=referer or BASE)
    dsoup = BeautifulSoup(html, "lxml")
    debug["detail_html_len"] = len(html)

    # ì„¤ëª…/ë³¸ë¬¸ ì˜ì—­ í›„ë³´
    nodes = find_description_nodes(dsoup) or [dsoup]
    extra = dsoup.select("div#main, div#primary, div#content, div.single, div.entry-content")
    nodes = (nodes + extra)[:8]

    # 1) ì‹¤ì œ DOMì˜ <img> + ê´‘ë²”ìœ„ data-* ì†ì„± ì»¤ë²„
    DATA_ATTR_HINTS = {
        "data-src", "data-original", "data-lazy", "data-lazy-src",
        "data-echo", "data-image", "data-img", "data-url", "data-srcset"
    }
    for n in nodes:
        for img in n.find_all("img"):
            # í‘œì¤€
            add_img_url(img.get("src"), "img.src")
            # srcset
            srcset = img.get("srcset")
            if srcset:
                for p in [p.strip() for p in srcset.split(",") if p.strip()]:
                    add_img_url(p.split()[0], "img.srcset")
            # í¬ê´„ì  data-*
            for k, v in img.attrs.items():
                if not v or not isinstance(v, str):
                    continue
                if k in DATA_ATTR_HINTS or k.startswith("data-"):
                    add_img_url(v, f"img.{k}")

    # 2) noscript ë‚´ <img> (lazy-load ëŒ€ì²´)
    if include_noscript:
        for n in nodes:
            for nos in n.find_all("noscript"):
                inner = BeautifulSoup(nos.get_text() or "", "lxml")
                for img in inner.find_all("img"):
                    add_img_url(img.get("src"), "noscript.img.src")
                    for k in ("data-src", "data-original", "data-lazy", "data-lazy-src"):
                        add_img_url(img.get(k), f"noscript.img.{k}")
                    sset = img.get("srcset")
                    if sset:
                        for p in [p.strip() for p in sset.split(",") if p.strip()]:
                            add_img_url(p.split()[0], "noscript.img.srcset")

    # 3) (ì˜µì…˜) video poster
    if include_video_poster:
        for v in dsoup.find_all("video"):
            add_img_url(v.get("poster"), "video.poster")

    # 4) (ì˜µì…˜) ë©”íƒ€ í´ë°±
    if include_meta:
        for m in dsoup.select('meta[property="og:image"], meta[name="twitter:image"]'):
            add_img_url(m.get("content"), "meta.og_or_twitter")
        for l in dsoup.select('link[rel="image_src"]'):
            add_img_url(l.get("href"), "link.image_src")

    # 5) (ì˜µì…˜) <script> ë‚´ ì´ë¯¸ì§€ URL í´ë°±
    if include_script:
        IMG_RE = re.compile(r'https?://[^\s\'"]+\.(?:jpg|jpeg|png|webp|gif|avif)\b', re.I)
        for sc in dsoup.find_all("script"):
            txt = sc.string or sc.get_text() or ""
            for m in IMG_RE.finditer(txt):
                add_img_url(m.group(0), "script.url")

    # 6) (ì˜µì…˜) JAVMOST í¬ìŠ¤í„° ì¶”ì •: https://img{1..5}.javmost.com/images/<CODE>.webp
    if poster_guess:
        pr = up.urlparse(view_url)
        host = pr.netloc.lower()
        if host.endswith("javmost.com"):
            slug = pathlib.Path(pr.path).parts[-1].strip("/") or ""
            mcode = re.search(r"([A-Za-z]+-?\d+)", slug)
            if mcode:
                code = mcode.group(1).upper().replace("--", "-")
                for n in ("3", "2", "1", "4", "5"):  # ê´€ì¸¡ìƒ 3ì´ ê°€ì¥ í”í•´ ìš°ì„ 
                    cand = f"https://img{n}.javmost.com/images/{code}.webp"
                    add_img_url(cand, f"poster.guess.img{n}")

    # 7) ë§ˆí¬ë‹¤ìš´ IMG ë§í¬ ì²˜ë¦¬ (í˜ì´ì§€ê°€ ë§ˆí¬ë‹¤ìš´ ì›ë¬¸ì¸ ê²½ìš°)
    for m in MD_IMG_LINK_RE.finditer(html or ""):
        img_url = m.group("img")
        if img_url:
            add_img_url(img_url, "md.img")

    # ì¤‘ë³µ ì œê±°
    seen, cands = set(), []
    for it in raw:
        u = (it["url"] or "").strip()
        if u and u not in seen:
            cands.append(it); seen.add(u)

    debug["candidate_count"] = len(cands)
    debug["candidates_sample"] = cands[:20]

    accepted, rejected = [], []

    # ì €ì¥ íŒŒì¼ëª… prefix (ì½”ë“œ/ìŠ¬ëŸ¬ê·¸ ê¸°ë°˜)
    slug = pathlib.Path(up.urlparse(view_url).path).parts[-1].strip("/") or "view"
    view_id = re.search(r"([A-Za-z]+-?\d+)", slug)
    base_prefix = view_id.group(1).upper() if view_id else "view"

    for i, item in enumerate(cands, 1):
        u, how = item["url"], item["how"]

        # a) ìì‚°/ê´‘ê³  ì œì™¸
        if is_probably_asset(u):
            rejected.append({"url": u, "reason": "asset_or_ad", "how": how})
            continue

        # b) .html ë“± ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ ì œì™¸ (ì•ˆì „ì¥ì¹˜)
        ext = pathlib.Path(up.urlparse(u).path).suffix.lower()
        if ext == ".html":
            rejected.append({"url": u, "reason": "html_viewer_not_allowed", "how": how})
            continue

        # c) ë„¤íŠ¸ì›Œí¬ ê²€ì‚¬: image/* ë§Œ í—ˆìš© + ìµœì†Œ ìš©ëŸ‰
        probe = head_or_small_get(u, s, referer=view_url)
        if not probe["ok"]:
            rejected.append({"url": u, "reason": f"not_image({probe['ct']})", "how": how})
            continue
        size_ok = (probe["size"] is None) or (probe["size"] >= MIN_BYTES)
        if not size_ok:
            rejected.append({"url": u, "reason": f"small({probe['size']})", "how": how})
            continue

        # d) í†µê³¼ â†’ URL ì¶œë ¥(í•„ìˆ˜), í•„ìš” ì‹œ ì €ì¥
        results.append(u)
        if download:
            name = pathlib.Path(up.urlparse(u).path).name or f"{base_prefix}_{i:02d}{ext_from_content_type(probe['ct'])}"
            dest = os.path.join(out_dir, name)
            os.makedirs(out_dir, exist_ok=True)
            try:
                with s.get(u, headers=make_headers(view_url), stream=True, timeout=40) as r:
                    r.raise_for_status()
                    with open(dest, "wb") as f:
                        for chunk in r.iter_content(8192):
                            if chunk: f.write(chunk)
                accepted.append({"url": u, "saved": dest, "ct": probe["ct"], "size": probe["size"], "how": how})
                time.sleep(0.08)
            except Exception as e:
                rejected.append({"url": u, "reason": f"download_error:{e}", "how": how})
        else:
            accepted.append({"url": u, "ct": probe["ct"], "size": probe["size"], "how": how})

    debug["accepted"] = accepted
    debug["rejected"] = rejected
    debug["returned_count"] = len(results)
    save_debug_json(out_dir, f"view_{base_prefix}", debug)
    return results

# -------------------- ì—”íŠ¸ë¦¬: í‚¤ì›Œë“œë¡œ ìƒì„¸ ì°¾ì•„ê°€ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ --------------------
def scrape_javmost_image_urls_by_keyword(
    keyword: str,
    out_dir: str = "downloads",
    download: bool = False,
    **extract_opts,                  # í•˜ìœ„ ì¶”ì¶œ ì˜µì…˜ ì „ë‹¬(include_meta, poster_guess ë“±)
) -> list[str]:
    """
    1) í‚¤ì›Œë“œë¥¼ ì½”ë“œë¡œ ì •ê·œí™” í›„ ìƒì„¸ í˜ì´ì§€ ì§í–‰ ì‹œë„
    2) ì‹¤íŒ¨ ì‹œ ì½”ë“œ ì ‘ë‘ íƒœê·¸ í˜ì´ì§€ì—ì„œ 'ì²« ë²ˆì§¸' ì—„ê²© ë§¤ì¹­ ê²°ê³¼ ì„ íƒ
    3) ìƒì„¸(view) í˜ì´ì§€ì—ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€'ë§Œ ì¶”ì¶œ(+ì„ íƒ ì €ì¥)
    """
    debug = {"keyword": keyword, "base": BASE, "mode": "by_keyword_visible_imgs", "min_bytes": MIN_BYTES}
    with requests.Session() as s:
        # 1~2) ìƒì„¸ URL/ì œëª© í•´ê²°
        view_url, title_text, steps = resolve_view_url_and_title(keyword, s)
        debug.update({"view_url": view_url, "title_text": title_text, "steps": steps})

        if not view_url or not title_text:
            debug["error"] = "no_result_view_link"
            save_debug_json(out_dir, f"keyword_{keyword}", debug)
            return []

        # 3) ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€'ë§Œ ì¶”ì¶œ
        urls = extract_visible_image_urls_from_view(
            view_url,
            out_dir=out_dir,
            download=download,
            session=s,
            referer=BASE,
            **extract_opts
        )

    # ìš”ì•½ ë””ë²„ê·¸
    save_debug_json(out_dir, f"keyword_{keyword}", {
        "keyword": keyword,
        "view_url": view_url,
        "download": download,
        "found_count": len(urls)
    })
    return urls

# -------------------- ì‹¤í–‰ ì˜ˆì‹œ --------------------
if __name__ == "__main__":
    # ì˜ˆ) ê²€ìƒ‰ì–´ë¡œ ìƒì„¸ ì°¾ì•„ê°€ì„œ 'ë³´ì´ëŠ” ì´ë¯¸ì§€' URLë§Œ ì¶œë ¥
    urls = scrape_javmost_image_urls_by_keyword(
        "JUR-539",            # ì˜ˆì‹œ í‚¤ì›Œë“œ
        out_dir="test_images",
        download=False,       # URLë§Œ(ì €ì¥ X)
        include_meta=True,    # ë©”íƒ€ í´ë°±
        poster_guess=True     # í¬ìŠ¤í„° ì¶”ì • (img*.javmost.com/images/<CODE>.webp)
        # include_script=True  # í•„ìš”ì‹œ ìŠ¤í¬ë¦½íŠ¸ ë‚´ URLê¹Œì§€ í´ë°±
    )
    print("URLS ON PAGE (IMG ONLY):", *urls, sep="\n")
